{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "import spacy\n",
    "import langdetect\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the data\n",
    "def clean_data(df):\n",
    "    # Create a dictionary to store the values\n",
    "    new_df = {\"label\": [], \"text\": []}\n",
    "\n",
    "    # Iterate over all rows in the dataset\n",
    "    for row in tqdm(range(len(df.loc[:, \"text\"].to_list()))):\n",
    "        # Initialize temporary array to store tokens\n",
    "        tmp_tokens = []\n",
    "\n",
    "        try:\n",
    "            # # Check whether the review is written in English or not\n",
    "            # if langdetect.detect(df.loc[row, \"text\"]) == \"en\" or True:\n",
    "                for token in nlp(df.loc[row, \"text\"]):\n",
    "                    # Set conditions to retain valuable information\n",
    "                    if (\n",
    "                        not token.is_stop  # remove stop-words\n",
    "                        and not token.is_punct  # remove punctuation\n",
    "                        and not token.like_num  # remove numbers\n",
    "                        and token.is_oov  # remove words that don't have a word vector\n",
    "                        and not token.is_space  # remove whitespaces\n",
    "                        and len(token) > 1  # remove single-letter words\n",
    "                        # Remove tokens that looks weird & not useful\n",
    "                        and not str(token).endswith(\"-\")\n",
    "                        and not str(token).endswith(\".\")\n",
    "                        and not any(\n",
    "                            substr in str(token)\n",
    "                            for substr in [\n",
    "                                \"---\",\n",
    "                                \"--\",\n",
    "                                \"/2\",\n",
    "                                \"/1\",\n",
    "                                \"20feb\",\n",
    "                                \"c17\",\n",
    "                                \"\\x92\",\n",
    "                                \"&\",\n",
    "                                \"%\",\n",
    "                                \"i.e.\",\n",
    "                                \"b+\",\n",
    "                                \"w/\",\n",
    "                                \"02:33:05\",\n",
    "                            ]\n",
    "                        )\n",
    "                        and not str(token).startswith(\"-\")\n",
    "                    ):\n",
    "                        # Get the lemma & lowercase the token\n",
    "                        token = token.lemma_.lower()\n",
    "                        if \"(\" in token:\n",
    "                            token = token.split(\"(\")\n",
    "                            tmp_tokens.append(token[0])\n",
    "                            tmp_tokens.append(token[1])\n",
    "                        elif token == \"orangy/\":\n",
    "                            token = \"orangy\"\n",
    "                        elif token == \".fruity\":\n",
    "                            token = \"fruity\"\n",
    "\n",
    "                        tmp_tokens.append(token)\n",
    "\n",
    "                # Append the corresponding label to the review\n",
    "                new_df[\"label\"].append(df.loc[row, \"label\"])\n",
    "\n",
    "                # Add all tokens from the review to the text\n",
    "                new_df[\"text\"].append(tmp_tokens)\n",
    "                # Reset the token array\n",
    "                tmp_tokens = []\n",
    "        except:\n",
    "            continue  # proceed to next row if an exception is raised\n",
    "        \n",
    "    # Return the new dataframe\n",
    "    return pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
