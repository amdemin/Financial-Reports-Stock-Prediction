{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "# retrieve text from PDF\n",
    "from tqdm.notebook import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm.notebook import tqdm\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import spacy  \n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import pdfplumber\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reports(pdf_texts):\n",
    "    \n",
    "    # Initialize a dictionary to store the joined sentences for each report.\n",
    "    joined_sentences = {}\n",
    "\n",
    "    # Iterate over each report in pdf_texts.\n",
    "    for report_name, report_text in tqdm(pdf_texts.items()): # 18 seconds\n",
    "\n",
    "        # Split the report text into sentences.\n",
    "        sentences = nlp(report_text).sents\n",
    "\n",
    "        # Initialize a list to hold the tokenized sentences for this report.\n",
    "        tokenized_report_sentences = []\n",
    "\n",
    "        # Iterate over each sentence.\n",
    "        for sentence in sentences:\n",
    "            # Tokenize, lemmatize, and remove stop words and punctuation.\n",
    "            tokenized = [token.lemma_ for token in sentence if not token.is_stop and not token.is_punct]\n",
    "            # Add the tokenized sentence to the list.\n",
    "            tokenized_report_sentences.append(tokenized)\n",
    "\n",
    "        # Join each tokenized sentence into a single string, and store them in a list.\n",
    "        joined_report_sentences = [' '.join(sentence) for sentence in tokenized_report_sentences]\n",
    "\n",
    "        # Add the joined sentences for this report to joined_sentences.\n",
    "        joined_sentences[report_name] = joined_report_sentences \n",
    "\n",
    "    return joined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "def analyze_sentiment_of_sentences_with_keywords(joined_sentences, keywords):\n",
    "      \n",
    "    # Initialize a dictionary to store the sentences and their sentiment scores for each report\n",
    "    sentiment_results_dict = {}\n",
    "\n",
    "    # Iterate over each report\n",
    "    for report_name, sentences in joined_sentences.items():\n",
    "        # Initialize a dictionary to store the sentiment analysis results for the current report\n",
    "        report_sentiment_results = {keyword: [] for keyword in keywords}\n",
    "\n",
    "        # Create a list to hold sentence chunks\n",
    "        sentence_chunks = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # If a sentence exceeds 512 characters, break it into chunks\n",
    "            if len(sentence) >= 512:\n",
    "                chunked_sentences = [sentence[i:i + 512] for i in range(0, len(sentence), 512)]\n",
    "                sentence_chunks.extend(chunked_sentences)\n",
    "            else:\n",
    "                sentence_chunks.append(sentence)\n",
    "\n",
    "        # Analyze the sentiment for each sentence chunk using BERT model\n",
    "        for chunk in sentence_chunks:\n",
    "            chunk_lower = chunk.lower()\n",
    "            for keyword in keywords:\n",
    "                if keyword in chunk_lower:\n",
    "                    inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "                    outputs = model(**inputs)\n",
    "                    probs = softmax(outputs.logits, dim=1)\n",
    "                    sentiment_result = {\n",
    "                        \"label\": \"positive\" if probs[0][1] > probs[0][0] else \"negative\",\n",
    "                        \"score\": probs[0][1].item()\n",
    "                    }\n",
    "                    \n",
    "                    #\n",
    "                    \n",
    "                    \n",
    "                    report_sentiment_results[keyword].append(sentiment_result)\n",
    "\n",
    "        # Add the results to the dictionary\n",
    "        sentiment_results_dict[report_name] = report_sentiment_results\n",
    "\n",
    "    return sentiment_results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9545e431bb418791850204224175cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  pdf_name  revenue_score  forecast_score  \\\n",
      "0  COMBINED-Q4-17-Shareholder-Letter-FINAL      -5.034062       -4.130627   \n",
      "1           FINAL-Q1-18-Shareholder-Letter      -3.673780       -3.678387   \n",
      "2           FINAL-Q1-19-Shareholder-Letter      -4.606944       -2.770117   \n",
      "3           FINAL-Q1-20-Shareholder-Letter      -5.988074       -3.691145   \n",
      "4           FINAL-Q1-21-Shareholder-Letter      -4.602832       -4.627137   \n",
      "\n",
      "   profit_score   polarity  \n",
      "0     -4.572766 -13.737455  \n",
      "1     -1.372522  -8.724689  \n",
      "2     -1.371546  -8.748607  \n",
      "3     -1.389740 -11.068960  \n",
      "4     -0.462943  -9.692912  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load nlp model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Load pdf texts from the pickle file\n",
    "    pdf_texts = pickle.load(open(\"pdf_texts.pkl\", \"rb\"))\n",
    "\n",
    "    # Tokenize the reports\n",
    "    joined_sentences = tokenize_reports(pdf_texts)\n",
    "    # Define keywords\n",
    "    keywords = ['revenue', 'forecast', 'profit']\n",
    "    # Analyze the sentiment of the sentences containing the keywords\n",
    "    baseline_keyword_polarity_dict = analyze_sentiment_of_sentences_with_keywords(joined_sentences, keywords)\n",
    "\n",
    "    # Initialize a dictionary to hold total scores for each keyword in each report\n",
    "    total_scores = {report: {keyword: 0 for keyword in keywords} for report in baseline_keyword_polarity_dict.keys()}\n",
    "\n",
    "    # Calculate total scores for each keyword in each report\n",
    "    for report_name, keywords_dict in baseline_keyword_polarity_dict.items():\n",
    "        for keyword, sentiments in keywords_dict.items():\n",
    "            for sentiment in sentiments:\n",
    "                # If the sentiment is POSITIVE, add the score\n",
    "                # If the sentiment is NEGATIVE, subtract the score\n",
    "                if sentiment['label'] == 'POSITIVE':\n",
    "                    total_scores[report_name][keyword] += sentiment['score']\n",
    "                else:\n",
    "                    total_scores[report_name][keyword] -= sentiment['score']\n",
    "\n",
    "    # Convert the total_scores to a DataFrame\n",
    "    baseline_keyword_polarity_df = pd.DataFrame(total_scores).T\n",
    "    baseline_keyword_polarity_df.reset_index(inplace=True)\n",
    "    baseline_keyword_polarity_df.columns = ['pdf_name', 'revenue_score', 'forecast_score', 'profit_score']\n",
    "    baseline_keyword_polarity_df['polarity'] = baseline_keyword_polarity_df['revenue_score'] + baseline_keyword_polarity_df['forecast_score'] + baseline_keyword_polarity_df['profit_score']\n",
    "    \n",
    "    print(baseline_keyword_polarity_df.head())\n",
    "    \n",
    "    # export dataframe to csv\n",
    "    baseline_keyword_polarity_df.to_csv('baseline_keyword_polarity.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shetty_python_3.10",
   "language": "python",
   "name": "shetty_python_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
