{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(f\"--user-agent={user_agent}\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_cookies(driver):\n",
    "    accept_cookies_button = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[@data-n-messaging-accept-cookies]\"))\n",
    "    )\n",
    "    accept_cookies_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_in(driver, email, password):\n",
    "    sign_in_link = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.LINK_TEXT, \"Sign In\"))\n",
    "    )\n",
    "    sign_in_link.click()\n",
    "\n",
    "\n",
    "        # Find the email input element by its ID\n",
    "    #email_input = WebDriverWait(driver, 20).until(\n",
    "    #    EC.presence_of_element_located((By.ID, \"enter-email\"))\n",
    "    \n",
    "\n",
    "    # Clear any existing text in the input field (optional, based on your use case)\n",
    "    #email_input.clear()\n",
    "\n",
    "    # Enter the email address into the input field\n",
    "    #email_address = \"soumyajit.saha@bayes.city.ac.uk\"  # Replace with the actual email address\n",
    "    #email_input.send_keys(email_address)\n",
    "\n",
    "\n",
    "    email_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"enter-email\"))\n",
    "    )\n",
    "      # Clear any existing text in the input field (optional, based on your use case)\n",
    "    email_input.clear()\n",
    "\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "# Find the \"Next\" button element by its ID\n",
    "    next_button = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"enter-email-next\"))\n",
    "    )\n",
    "\n",
    "    # Click the \"Next\" button\n",
    "    next_button.click()\n",
    "\n",
    "\n",
    "\n",
    "    # Find the \"SSO Sign in\" link element by its href attribute\n",
    "    sso_sign_in_link = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, 'sso.ft.com')]\"))\n",
    "    )\n",
    "\n",
    "    # Click the \"SSO Sign in\" link\n",
    "    sso_sign_in_link.click()\n",
    "\n",
    "        # Find the email input element by its ID\n",
    "    email_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"userNameInput\"))\n",
    "    )\n",
    "\n",
    "    # Enter the email address into the email input field\n",
    "\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Find the password input element by its ID\n",
    "    password_input = driver.find_element(By.ID, \"passwordInput\")\n",
    "\n",
    "    # Enter the password into the password input field\n",
    "   \n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Find the \"Sign in\" span element by its ID\n",
    "    sign_in_button = driver.find_element(By.ID, \"submitButton\")\n",
    "\n",
    "    # Click the \"Sign in\" span element\n",
    "    sign_in_button.click()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_topic(driver, topic):\n",
    "    search_button = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//a[contains(@class, 'o-header__top-icon-link--search')]\"))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", search_button)\n",
    "    driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "\n",
    "    search_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//input[@id='o-header-search-term-primary']\"))\n",
    "    )\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(topic)\n",
    "\n",
    "    search_button = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//button[@class='o-header__search-submit']\"))\n",
    "    )\n",
    "    search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_content(soup, class_name):\n",
    "    element = soup.find(class_=class_name)\n",
    "    return element.get_text() if element else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_urls(driver, word='netflix'):\n",
    "    netflix_articles = []\n",
    "\n",
    "    while True:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"o-teaser__content\"))\n",
    "        )\n",
    "        teaser_elements = driver.find_elements(By.CLASS_NAME, \"o-teaser__content\")\n",
    "        for teaser in teaser_elements:\n",
    "            heading_element = teaser.find_element(By.CLASS_NAME, \"o-teaser__heading\")\n",
    "            heading_text = heading_element.text.lower()\n",
    "            if word in heading_text:\n",
    "                link_element = heading_element.find_element(By.TAG_NAME, \"a\")\n",
    "                article_url = link_element.get_attribute(\"href\")\n",
    "                \n",
    "                netflix_articles.append(article_url)\n",
    "        \n",
    "        # Rest of the code\n",
    "        error_message = \"Sorry, FT.com does not serve more than 1000 results\"\n",
    "        if error_message in driver.page_source:\n",
    "            break\n",
    "        # print(\"one page\")debug statement\n",
    "        \n",
    "        next_page_arrow = driver.find_element(By.CSS_SELECTOR, \".search-pagination__next-page\")\n",
    "        next_page_arrow.click()\n",
    "\n",
    "    return netflix_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_to_dataframe(driver, netflix_articles):\n",
    "    scraped_data = []\n",
    "    error_log = []\n",
    "\n",
    "    # Loop through the URLs\n",
    "    for url in netflix_articles:\n",
    "        try:\n",
    "            # Open the URL using ChromeDriver\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after waiting for a bit to ensure it's fully loaded\n",
    "            driver.implicitly_wait(5)\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Parse the page source using Beautiful Soup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Find the author's link\n",
    "            try:\n",
    "                author_link = soup.find(\"a\", class_=\"n-content-tag--author\")\n",
    "                author_name = author_link.text if author_link else \"Unknown Author\"\n",
    "            except Exception as author_err:\n",
    "                author_name = \"Error extracting author\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Author\", \"error\": str(author_err)})\n",
    "            \n",
    "            # Find the heading element\n",
    "            try:\n",
    "                heading_element = soup.find(\"h1\", class_=\"o-topper__headline\")\n",
    "                heading = heading_element.text if heading_element else \"Unknown Heading\"\n",
    "            except Exception as heading_err:\n",
    "                heading = \"Error extracting heading\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Heading\", \"error\": str(heading_err)})\n",
    "            \n",
    "            # Find the timestamp element\n",
    "            try:\n",
    "                timestamp_element = soup.find(\"time\", class_=\"article-info__timestamp\")\n",
    "                timestamp = timestamp_element['datetime'] if timestamp_element else \"Unknown Timestamp\"\n",
    "                date, time = timestamp.split('T')\n",
    "            except Exception as timestamp_err:\n",
    "                date = \"Unknown Date\"\n",
    "                time = \"Unknown Time\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Timestamp\", \"error\": str(timestamp_err)})\n",
    "            \n",
    "            # Find the article content element\n",
    "            try:\n",
    "                article_content_element = soup.find(\"div\", class_=\"article__content-body\")\n",
    "                \n",
    "                # Extract the full article text\n",
    "                article_text = \"\"\n",
    "                for paragraph in article_content_element.find_all(\"p\"):\n",
    "                    article_text += paragraph.get_text() + \"\\n\"\n",
    "            except Exception as article_err:\n",
    "                article_text = \"Error extracting article text\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Article\", \"error\": str(article_err)})\n",
    "            \n",
    "            # Store the scraped data in a dictionary\n",
    "            scraped_data.append({\n",
    "                \"url\": url,\n",
    "                \"author\": author_name,\n",
    "                \"heading\": heading,\n",
    "                \"date\": date,\n",
    "                \"time\": time[:-5],\n",
    "                \"article_text\": article_text\n",
    "            })\n",
    "        \n",
    "        except Exception as page_err:\n",
    "            error_log.append({\"url\": url, \"field\": \"Page\", \"error\": str(page_err)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_new = pd.DataFrame(scraped_data)\n",
    "\n",
    "   \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dataframe(df_new):\n",
    "    df_cleaned = df_new[\n",
    "        (df_new['author'] != 'Unknown Author') &\n",
    "        (df_new['heading'] != 'Unknown Heading') &\n",
    "        (df_new['date'] != 'Unknown') &\n",
    "        (df_new['time'] != 'ime')\n",
    "    ]\n",
    "\n",
    "    df_sorted = df_cleaned.sort_values(by='date', ascending=False)\n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_sign_in(webpage,email_address, password):\n",
    "    driver = initialize_driver()\n",
    "    driver.get(webpage)\n",
    "    accept_cookies(driver)\n",
    "    sign_in(driver, email_address, password)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    email_address = \"xxx@bayes.city.ac.uk\" # put email id \n",
    "    password = \"xxxx\"# put actual password\n",
    "    \n",
    "\n",
    "    driver = initialize_and_sign_in(email_address, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "Netflix is ok for url\n",
      "Netflix is ok for scraped df\n",
      "Netflix is ok for scraped df sort\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "Netflix Inc is ok for url\n",
      "Netflix Inc is ok for scraped df\n",
      "Netflix Inc is ok for scraped df sort\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "Netflix quaterly result is ok for url\n",
      "Netflix quaterly result is ok for scraped df\n",
      "Netflix quaterly result is ok for scraped df sort\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "one page\n",
      "Netflix financial result is ok for url\n",
      "Netflix financial result is ok for scraped df\n",
      "Netflix financial result is ok for scraped df sort\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "\n",
    "    topics = [\"Netflix\", \"Netflix Inc\", \"Netflix quaterly result\", \"Netflix financial result\"]\n",
    "    dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    for topic in topics:\n",
    "        search_topic(driver, topic)\n",
    "        article_urls = scrape_article_urls(driver)\n",
    "        print(f\"{topic} is ok for url\")\n",
    "        df=scrape_data_to_dataframe(driver, article_urls)\n",
    "        print(f\"{topic} is ok for scraped df\")\n",
    "\n",
    "        df_sorted=sort_dataframe(df)\n",
    "        print(f\"{topic} is ok for scraped df sort\")\n",
    "        \n",
    "        # Save the cleaned and sorted dataframe\n",
    "        file_name = f'ft_articles_{topic}.csv'\n",
    "        df_sorted.to_csv(file_name, index=False)\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df_sorted)\n",
    "\n",
    "    # Merge the dataframes to remove duplicates\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=['url'])\n",
    "\n",
    "    # Save the merged dataframe\n",
    "    merged_file_name = 'ft_articles_merged.csv'\n",
    "    merged_df.to_csv(merged_file_name, index=False)\n",
    "\n",
    "    #driver.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "businessanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
