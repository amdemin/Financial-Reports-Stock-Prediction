{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(f\"--user-agent={user_agent}\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_cookies(driver):\n",
    "    accept_cookies_button = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[@data-n-messaging-accept-cookies]\"))\n",
    "    )\n",
    "    accept_cookies_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_in(driver, email, password):\n",
    "    sign_in_link = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.LINK_TEXT, \"Sign In\"))\n",
    "    )\n",
    "    sign_in_link.click()\n",
    "\n",
    "    email_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"enter-email\"))\n",
    "    )\n",
    "      # Clear any existing text in the input field (optional, based on your use case)\n",
    "    email_input.clear()\n",
    "\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    next_button = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.ID, \"enter-email-next\"))\n",
    "    )\n",
    "\n",
    "    # Click the \"Next\" button\n",
    "    next_button.click()\n",
    "\n",
    "    # Find the \"SSO Sign in\" link element by its href attribute\n",
    "    sso_sign_in_link = WebDriverWait(driver, 20).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, 'sso.ft.com')]\"))\n",
    "    )\n",
    "\n",
    "    # Click the \"SSO Sign in\" link\n",
    "    sso_sign_in_link.click()\n",
    "\n",
    "        # Find the email input element by its ID\n",
    "    email_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.ID, \"userNameInput\"))\n",
    "    )\n",
    "\n",
    "    # Enter the email address into the email input field\n",
    "    email_input.send_keys(email)\n",
    "\n",
    "    # Find the password input element by its ID\n",
    "    password_input = driver.find_element(By.ID, \"passwordInput\")\n",
    "\n",
    "    # Enter the password into the password input field\n",
    "    password_input.send_keys(password)\n",
    "\n",
    "    # Find the \"Sign in\" span element by its ID\n",
    "    sign_in_button = driver.find_element(By.ID, \"submitButton\")\n",
    "\n",
    "    # Click the \"Sign in\" span element\n",
    "    sign_in_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_topic(driver, topic):\n",
    "    search_button = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//a[contains(@class, 'o-header__top-icon-link--search')]\"))\n",
    "    )\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", search_button)\n",
    "    driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "\n",
    "    search_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//input[@id='o-header-search-term-primary']\"))\n",
    "    )\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(topic)\n",
    "\n",
    "    search_button = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//button[@class='o-header__search-submit']\"))\n",
    "    )\n",
    "    search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(soup, class_name):\n",
    "    element = soup.find(class_=class_name)\n",
    "    return element.get_text() if element else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_urls(driver, word='netflix'):\n",
    "    netflix_articles = []\n",
    "\n",
    "    while True:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"o-teaser__content\"))\n",
    "        )\n",
    "        teaser_elements = driver.find_elements(By.CLASS_NAME, \"o-teaser__content\")\n",
    "        for teaser in teaser_elements:\n",
    "            heading_element = teaser.find_element(By.CLASS_NAME, \"o-teaser__heading\")\n",
    "            heading_text = heading_element.text.lower()\n",
    "            if word in heading_text:\n",
    "                link_element = heading_element.find_element(By.TAG_NAME, \"a\")\n",
    "                article_url = link_element.get_attribute(\"href\")\n",
    "                \n",
    "                netflix_articles.append(article_url)\n",
    "        \n",
    "        # Rest of the code\n",
    "        error_message = \"Sorry, FT.com does not serve more than 1000 results\"\n",
    "        if error_message in driver.page_source:\n",
    "            break\n",
    "        # print(\"one page\")debug statement\n",
    "        \n",
    "        next_page_arrow = driver.find_element(By.CSS_SELECTOR, \".search-pagination__next-page\")\n",
    "        next_page_arrow.click()\n",
    "\n",
    "    return netflix_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data_to_dataframe(driver, netflix_articles):\n",
    "    scraped_data = []\n",
    "    error_log = []\n",
    "\n",
    "    # Loop through the URLs\n",
    "    for url in netflix_articles:\n",
    "        try:\n",
    "            # Open the URL using ChromeDriver\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after waiting for a bit to ensure it's fully loaded\n",
    "            driver.implicitly_wait(5)\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Parse the page source using Beautiful Soup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Find the author's link\n",
    "            try:\n",
    "                author_link = soup.find(\"a\", class_=\"n-content-tag--author\")\n",
    "                author_name = author_link.text if author_link else \"Unknown Author\"\n",
    "            except Exception as author_err:\n",
    "                author_name = \"Error extracting author\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Author\", \"error\": str(author_err)})\n",
    "            \n",
    "            # Find the heading element\n",
    "            try:\n",
    "                heading_element = soup.find(\"h1\", class_=\"o-topper__headline\")\n",
    "                heading = heading_element.text if heading_element else \"Unknown Heading\"\n",
    "            except Exception as heading_err:\n",
    "                heading = \"Error extracting heading\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Heading\", \"error\": str(heading_err)})\n",
    "            \n",
    "            # Find the timestamp element\n",
    "            try:\n",
    "                timestamp_element = soup.find(\"time\", class_=\"article-info__timestamp\")\n",
    "                timestamp = timestamp_element['datetime'] if timestamp_element else \"Unknown Timestamp\"\n",
    "                date, time = timestamp.split('T')\n",
    "            except Exception as timestamp_err:\n",
    "                date = \"Unknown Date\"\n",
    "                time = \"Unknown Time\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Timestamp\", \"error\": str(timestamp_err)})\n",
    "            \n",
    "            # Find the article content element\n",
    "            try:\n",
    "                article_content_element = soup.find(\"div\", class_=\"article__content-body\")\n",
    "                \n",
    "                # Extract the full article text\n",
    "                article_text = \"\"\n",
    "                for paragraph in article_content_element.find_all(\"p\"):\n",
    "                    article_text += paragraph.get_text() + \"\\n\"\n",
    "            except Exception as article_err:\n",
    "                article_text = \"Error extracting article text\"\n",
    "                error_log.append({\"url\": url, \"field\": \"Article\", \"error\": str(article_err)})\n",
    "            \n",
    "            # Store the scraped data in a dictionary\n",
    "            scraped_data.append({\n",
    "                \"url\": url,\n",
    "                \"author\": author_name,\n",
    "                \"heading\": heading,\n",
    "                \"date\": date,\n",
    "                \"time\": time[:-5],\n",
    "                \"article_text\": article_text\n",
    "            })\n",
    "        \n",
    "        except Exception as page_err:\n",
    "            error_log.append({\"url\": url, \"field\": \"Page\", \"error\": str(page_err)})\n",
    "\n",
    "    df_new = pd.DataFrame(scraped_data)\n",
    "   \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dataframe(df_new):\n",
    "    df_cleaned = df_new[\n",
    "        (df_new['author'] != 'Unknown Author') &\n",
    "        (df_new['heading'] != 'Unknown Heading') &\n",
    "        (df_new['date'] != 'Unknown') &\n",
    "        (df_new['time'] != 'ime')\n",
    "    ]\n",
    "\n",
    "    df_sorted = df_cleaned.sort_values(by='date', ascending=False)\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_and_sign_in(webpage, email_address, password):\n",
    "    driver = initialize_driver()\n",
    "    driver.get(webpage)\n",
    "    accept_cookies(driver)\n",
    "    sign_in(driver, email_address, password)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    webpage = \"https://www.ft.com/\"            # put the Financial Times webpage\n",
    "    email_address = \"example@bayes.city.ac.uk\" # put your email address\n",
    "    password = \"example\"                       # put your password\n",
    "    \n",
    "    # Initialize the driver and sign in\n",
    "    driver = initialize_and_sign_in(webpage, email_address, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to run the code above ans sign in before running this code\n",
    "# 21 min 50 sec to scrape Financial Times articles \n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    topics = [\"Netflix\", \"Netflix Inc\", \"Netflix quarterly result\", \"Netflix financial result\"]\n",
    "    dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    for topic in topics:\n",
    "        search_topic(driver, topic)\n",
    "        article_urls = scrape_article_urls(driver)\n",
    "        print(f\"{topic} is ok for url\")\n",
    "        df=scrape_data_to_dataframe(driver, article_urls)\n",
    "        print(f\"{topic} is ok for scraped df\")\n",
    "\n",
    "        df_sorted=sort_dataframe(df)\n",
    "        print(f\"{topic} is ok for scraped df sort\")\n",
    "        \n",
    "        # Save the cleaned and sorted dataframe\n",
    "        file_name = f'ft_articles_{topic}.csv'\n",
    "        df_sorted.to_csv(file_name, index=False)\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df_sorted)\n",
    "\n",
    "    # Merge the dataframes to remove duplicates\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=['url'])\n",
    "\n",
    "    # Save the merged dataframe\n",
    "    merged_file_name = 'ft_articles_merged.csv'\n",
    "    merged_df.to_csv(merged_file_name, index=False)\n",
    "\n",
    "    driver.quit()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "businessanalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
