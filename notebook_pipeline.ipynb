{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all necessary libraries and local modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Load local preprocesing modules\n",
    "from module_text_blocks import split_text_into_blocks, clean_text_blocks\n",
    "from module_process_pdf import process_pdf\n",
    "from module_scrape_pdf import scrape_pdf\n",
    "from module_functions import normalise_score\n",
    "\n",
    "# Load local model modules\n",
    "from module_sentiment_textblob import calculate_textblob_polarity\n",
    "from module_sentiment_google import calculate_google_polarity\n",
    "from module_sentiment_amazon import calculate_amazon_polarity\n",
    "from module_sentiment_openai import calculate_openai_polarity\n",
    "\n",
    "# add two model modules later (BERT and Roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_start_time = time.time()\n",
    "\n",
    "# Load historic models min and max values for normalisation\n",
    "historic_df = pd.read_csv(\"Src/historic_min_max.csv\", sep=\"\\t\")\n",
    "\n",
    "# Models with corresponding local functions to calculate sentiment score\n",
    "models = {\n",
    "    \"TextBlob\": calculate_textblob_polarity,\n",
    "    \"Google\": calculate_google_polarity,\n",
    "    \"Amazon\": calculate_amazon_polarity,\n",
    "    \"OpenAI\": calculate_openai_polarity\n",
    "}\n",
    "\n",
    "# scrape pdf from Netflix website\n",
    "Netflix_scraping_page = \"https://ir.netflix.net/financials/quarterly-earnings/default.aspx\"\n",
    "# change to scrape_all=False to only scrape the latest pdf, otherwise all pdfs will be scraped\n",
    "scrape_pdf(Netflix_scraping_page, scrape_all=False)\n",
    "\n",
    "# get all downloaded files from the dedicated folder\n",
    "pipeline_files = ['PipelineFiles/' + file for file in os.listdir('PipelineFiles') if file.endswith('.pdf')]\n",
    "\n",
    "# Transform pdf files into texts and headings and store them as dictionaries\n",
    "pdf_texts, pdf_headings, pdf_headings_context = process_pdf(pipeline_files)\n",
    "\n",
    "# Iterate over pdf texts\n",
    "for pdf_file in pdf_texts:\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Get text, headings and headings context\n",
    "        text = pdf_texts[pdf_file]\n",
    "        headings = pdf_headings[pdf_file]\n",
    "        headings_context = pdf_headings_context[pdf_file]\n",
    "\n",
    "        # Split text into blocks\n",
    "        text_blocks = split_text_into_blocks(text, headings, headings_context)\n",
    "\n",
    "        # Clean text blocks\n",
    "        text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "        # Iterate over models\n",
    "        for model_name, model_function in models.items():\n",
    "            \n",
    "            # set start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Calculate sentiment score\n",
    "            sentiment_score = model_function(text_blocks, headings, text)\n",
    "            \n",
    "            # Normalise sentiment score (except for OpenAI) \n",
    "            if model_name != \"OpenAI\": \n",
    "                sentiment_score = normalise_score(sentiment_score, historic_df[historic_df[\"Name\"] == model_name][\"Min\"].values[0], historic_df[historic_df[\"Name\"] == model_name][\"Max\"].values[0], historic_df.iloc[0, 1], historic_df.iloc[0, 2])\n",
    "        \n",
    "            # set end time\n",
    "            end_time = time.time()\n",
    "            # print sentiment score\n",
    "            print(f\"{pdf_file} {model_name} sentiment score: {sentiment_score}\")\n",
    "            # print processing time\n",
    "            print(f\"Time taken for {model_name} to process text: \", end_time - start_time)#\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "main_end_time = time.time()\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(f\"Total time taken for full pipeline to run: \", main_end_time - main_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Full Pipeline (No Scraping & PDF Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest shareholder text and headings from the pickle file\n",
    "pdf_texts = pickle.load(open(\"PipelineFiles/pdf_texts.pickle\", \"rb\"))                        # Texts are extracted from the pdf\n",
    "pdf_headings = pickle.load(open(\"PipelineFiles/pdf_headings.pickle\", \"rb\"))                  # Headings are extracted from the pdf text\n",
    "pdf_headings_context = pickle.load(open(\"PipelineFiles/pdf_headings_context.pickle\", \"rb\"))  # Surrounding text of headings helps to identify headings correctly and avoid duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historic models min and max values for normalisation\n",
    "historic_df = pd.read_csv(\"Src/historic_min_max.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL-Q2-23-Shareholder-Letter TextBlob sentiment score: 0.6603495902734645\n",
      "Time taken for TextBlob to process text:  0.040688514709472656\n",
      "FINAL-Q2-23-Shareholder-Letter Google sentiment score: 0.39000000804662704\n",
      "Time taken for Google to process text:  3.8614885807037354\n",
      "FINAL-Q2-23-Shareholder-Letter Amazon sentiment score: 0.5343444300341209\n",
      "Time taken for Amazon to process text:  18.47450304031372\n",
      "FINAL-Q2-23-Shareholder-Letter OpenAI sentiment score: 0.25\n",
      "Time taken for OpenAI to process text:  14.608911275863647\n",
      " \n",
      " \n",
      "Time taken for semi-full pipeline to process all texts:  36.98966908454895\n"
     ]
    }
   ],
   "source": [
    "# Models with correspondings local functions to calculate sentiment score\n",
    "models = {\n",
    "    \"TextBlob\": calculate_textblob_polarity,\n",
    "    \"Google\": calculate_google_polarity,\n",
    "    \"Amazon\": calculate_amazon_polarity,\n",
    "    \"OpenAI\": calculate_openai_polarity\n",
    "}\n",
    "\n",
    "main_start_time = time.time()\n",
    "\n",
    "# Iterate over pdf files\n",
    "for pdf_file in pdf_texts:\n",
    "\n",
    "    try:\n",
    "\n",
    "        text = pdf_texts[pdf_file]\n",
    "        headings = pdf_headings[pdf_file]\n",
    "        headings_context = pdf_headings_context[pdf_file]\n",
    "\n",
    "        # Split text into blocks\n",
    "        text_blocks = split_text_into_blocks(text, headings, headings_context)\n",
    "\n",
    "        # Clean text blocks\n",
    "        text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "        # Iterate over models\n",
    "        for model_name, model_function in models.items():\n",
    "            \n",
    "            # set start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Calculate sentiment score\n",
    "            sentiment_score = model_function(text_blocks, headings, text)\n",
    "            \n",
    "            # Normalise sentiment score (except for OpenAI) \n",
    "            if model_name != \"OpenAI\": \n",
    "                sentiment_score = normalise_score(sentiment_score, historic_df[historic_df[\"Name\"] == model_name][\"Min\"].values[0], historic_df[historic_df[\"Name\"] == model_name][\"Max\"].values[0], historic_df.iloc[0, 1], historic_df.iloc[0, 2])\n",
    "        \n",
    "            # set end time\n",
    "            end_time = time.time()\n",
    "            # print sentiment score\n",
    "            print(f\"{pdf_file} {model_name} sentiment score: {sentiment_score}\")\n",
    "            # print processing time\n",
    "            print(f\"Time taken for {model_name} to process text: \", end_time - start_time)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "main_end_time = time.time()\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(f\"Time taken for semi-full pipeline to run: \", main_end_time - main_start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARP_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
