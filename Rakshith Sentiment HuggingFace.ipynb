{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load local modules\n",
    "from module_text_blocks import split_text_into_blocks, clean_text_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf text and headings from the pickle file\n",
    "pdf_texts = pickle.load(open(\"pdf_texts.pkl\", \"rb\"))\n",
    "pdf_headings = pickle.load(open(\"pdf_headings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('COMBINED-Q4-17-Shareholder-Letter-FINAL',\n",
       "  ['Q4 Results',\n",
       "   'Forecast',\n",
       "   'Content',\n",
       "   'Product and Partnerships',\n",
       "   'Competition',\n",
       "   'Free Cash Flow and Capital Structure',\n",
       "   'Board of Directors',\n",
       "   'Summary',\n",
       "   '22, 2018 Earnings Interview, 3pm PST']),\n",
       " ('FINAL-Q1-18-Shareholder-Letter',\n",
       "  ['Q1 Results and Q2 Forecast',\n",
       "   'Content',\n",
       "   'Marketing',\n",
       "   'Product and Partnerships',\n",
       "   'Free Cash Flow and Capital Structure',\n",
       "   'Board of Directors',\n",
       "   'Reference',\n",
       "   'Earnings Video Interview, 3pm PST']),\n",
       " ('FINAL-Q1-19-Shareholder-Letter',\n",
       "  ['Q1 Results and Q2 Forecast',\n",
       "   'Content',\n",
       "   'Product and Partnerships',\n",
       "   'Competition',\n",
       "   'Cash Flow and Capital Structure',\n",
       "   'Reference',\n",
       "   'Appendix April 16, 2019 Earnings Interview, 3pm PST']),\n",
       " ('FINAL-Q1-20-Shareholder-Letter',\n",
       "  ['(€1',\n",
       "   'Q1 Results and Q2 Forecast',\n",
       "   'Content',\n",
       "   'Product',\n",
       "   'Cash Flow and Capital Structure',\n",
       "   'Reference']),\n",
       " ('FINAL-Q1-21-Shareholder-Letter',\n",
       "  ['Q1 Results and Q2 Forecast',\n",
       "   'Content',\n",
       "   'Product',\n",
       "   'Competition',\n",
       "   'Cash Flow and Capital Structure',\n",
       "   'Environmental, Social, and Governance (ESG)',\n",
       "   'Reference'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pdf_headings.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2214ca94cb4b55a66e75cdd825126a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_labels = {}\n",
    "positive_sentiment_probs = {}\n",
    "negative_sentiment_probs = {}\n",
    "\n",
    "for pdf_name in tqdm(pdf_texts):\n",
    "    # Input text containing Q4 financial results\n",
    "    # input_text = \"We had a beautiful Q4, completing a great year as internet TV expands globally. In 2017, we grew streaming revenue 36% to over $11 billion, added 24 million new memberships (compared to 19 million in 2016), achieved for the first time a full-year positive international contribution profit, and more than doubled global operating income. Average paid streaming memberships rose 25% year over year in Q4. Combined with a 9% increase in ASP, global streaming revenue growth amounted to 35%. Operating income of $245 million (7.5% margin) vs. $154 million prior year (6.2% margin) was slightly above our $238 million forecast. Operating margin for FY17 was 7.2%, on target with our goal at the beginning of this year. EPS was $0.41 vs. $0.15 last year and met our forecast of $0.41. There were several below the line items that affected net income, including a pre-tax $26 million non-cash unrealized loss from F/X remeasurement on our Eurobond. Our tax rate was helped by a $66 million foreign tax benefit, which partially offset a revaluation of our deferred tax assets and the impact from the mandatory deemed repatriation of accumulated foreign earnings related to the recent US tax reform. In Q4, we registered global net adds of 8.3 million, the highest quarter in our history and up 18% vs. last year’s record 7.05 million net adds. This exceeded our 6.3m forecast due primarily to stronger than expected acquisition fueled by our original content slate and the ongoing global adoption of internet entertainment. Geographically, the outperformance vs. guidance was broad-based. In the US, memberships rose by 2.0 million (vs. forecast of 1.25m) bringing total FY17 net adds to 5.3 million. ASP rose 5% year-over-year. Domestic contribution profit increased 5% year-over-year although contribution margin of 34.4% declined both on a year-over-year and sequential basis due to the marketing spend we noted in last quarter’s investor letter. Internationally, we added 6.36 million memberships (compared with guidance of 5.05m), a new record for quarterly net adds for this segment. Excluding a F/X impact of +$43 million, international revenue and ASP grew 59% and 12% year over year, respectively. The increase in ASP reflects price adjustments in a wide variety of our markets over the course of 2017. With contribution profit of $227 million in 2017 (4.5% contribution margin), the international segment delivered its first full year of positive contribution profit in our history. We took a $39m non-cash charge in Q4 for unreleased content we’ve decided not to move forward with. This charge was recognized in content expense in cost of revenues. Despite this unexpected expense, we slightly exceeded our contribution profit and operating income forecast due to our stronger than expected member growth and the timing of international content spend.\"\n",
    "\n",
    "    # pdf_name = \"FINAL-Q1-19-Shareholder-Letter\"\n",
    "\n",
    "    text = pdf_texts[pdf_name]\n",
    "    headings = pdf_headings[pdf_name]\n",
    "\n",
    "    # # split the text into blocks based on the headings\n",
    "    # text_blocks = split_text_into_blocks(text, headings)\n",
    "        \n",
    "    # # clean the text blocks\n",
    "    # text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "    # final_prompt = ''\n",
    "    # for heading, text in text_blocks.items():\n",
    "    #     final_prompt += heading + ': ' + text + \" \"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Chunk the input tensor into smaller parts\n",
    "    max_chunk_size = 512\n",
    "    chunked_input_ids = []\n",
    "    for i in range(0, inputs.input_ids.size(1), max_chunk_size):\n",
    "        start = i\n",
    "        end = min(i + max_chunk_size, inputs.input_ids.size(1))\n",
    "        chunked_input_ids.append(inputs.input_ids[:, start:end])\n",
    "\n",
    "    # Make predictions for each chunk\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunked_input_ids:\n",
    "            outputs = model(input_ids=chunk)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            probs_list.append(probs)\n",
    "\n",
    "    # Combine the probabilities for all chunks\n",
    "    combined_probs = torch.cat(probs_list, dim=0)\n",
    "\n",
    "    # Calculate the average probability for each sentiment class\n",
    "    average_probs = combined_probs.mean(dim=0)\n",
    "    positive_sentiment_prob = average_probs[1].item()\n",
    "    negative_sentiment_prob = average_probs[0].item()\n",
    "\n",
    "    # Determine the sentiment label based on the probability\n",
    "    sentiment_label = \"Positive\" if positive_sentiment_prob > negative_sentiment_prob else \"Negative\"\n",
    "\n",
    "    # Store the results\n",
    "    sentiment_labels[pdf_name] = sentiment_label\n",
    "    positive_sentiment_probs[pdf_name] = positive_sentiment_prob\n",
    "    negative_sentiment_probs[pdf_name] = negative_sentiment_prob\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Input Text:\", final_prompt)\n",
    "    # print(\"Sentiment Label:\", sentiment_label)\n",
    "    # print(\"Positive Sentiment Probability:\", positive_sentiment_prob)\n",
    "    # print(\"Negative Sentiment Probability:\", negative_sentiment_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_sentiment_labels_df = pd.DataFrame(list(sentiment_labels.items()), columns=['pdf_name', 'sentiment_label'])\n",
    "bart_positive_sentiment_probs_df = pd.DataFrame(list(positive_sentiment_probs.items()), columns=['pdf_name', 'positive_sentiment_prob'])\n",
    "bart_negative_sentiment_probs_df = pd.DataFrame(list(negative_sentiment_probs.items()), columns=['pdf_name', 'negative_sentiment_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv file\n",
    "bart_sentiment_labels_df.to_csv(\"Scores/bart_sentiment_labels.csv\", index=False)\n",
    "bart_positive_sentiment_probs_df.to_csv(\"Scores/bart_positive_sentiment_probs.csv\", index=False)\n",
    "bart_negative_sentiment_probs_df.to_csv(\"Scores/bart_negative_sentiment_probs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROBERTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained RoBERTa model and tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77033c6d6af245c181c9521d5a3c9b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6136 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "roberta_sentiment_labels = {}\n",
    "roberta_positive_sentiment_probs = {}\n",
    "roberta_negative_sentiment_probs = {}\n",
    "\n",
    "for pdf_name in tqdm(pdf_texts):\n",
    "    # Input text containing Q4 financial results\n",
    "    # input_text = \"We had a beautiful Q4, completing a great year as internet TV expands globally. In 2017, we grew streaming revenue 36% to over $11 billion, added 24 million new memberships (compared to 19 million in 2016), achieved for the first time a full-year positive international contribution profit, and more than doubled global operating income. Average paid streaming memberships rose 25% year over year in Q4. Combined with a 9% increase in ASP, global streaming revenue growth amounted to 35%. Operating income of $245 million (7.5% margin) vs. $154 million prior year (6.2% margin) was slightly above our $238 million forecast. Operating margin for FY17 was 7.2%, on target with our goal at the beginning of this year. EPS was $0.41 vs. $0.15 last year and met our forecast of $0.41. There were several below the line items that affected net income, including a pre-tax $26 million non-cash unrealized loss from F/X remeasurement on our Eurobond. Our tax rate was helped by a $66 million foreign tax benefit, which partially offset a revaluation of our deferred tax assets and the impact from the mandatory deemed repatriation of accumulated foreign earnings related to the recent US tax reform. In Q4, we registered global net adds of 8.3 million, the highest quarter in our history and up 18% vs. last year’s record 7.05 million net adds. This exceeded our 6.3m forecast due primarily to stronger than expected acquisition fueled by our original content slate and the ongoing global adoption of internet entertainment. Geographically, the outperformance vs. guidance was broad-based. In the US, memberships rose by 2.0 million (vs. forecast of 1.25m) bringing total FY17 net adds to 5.3 million. ASP rose 5% year-over-year. Domestic contribution profit increased 5% year-over-year although contribution margin of 34.4% declined both on a year-over-year and sequential basis due to the marketing spend we noted in last quarter’s investor letter. Internationally, we added 6.36 million memberships (compared with guidance of 5.05m), a new record for quarterly net adds for this segment. Excluding a F/X impact of +$43 million, international revenue and ASP grew 59% and 12% year over year, respectively. The increase in ASP reflects price adjustments in a wide variety of our markets over the course of 2017. With contribution profit of $227 million in 2017 (4.5% contribution margin), the international segment delivered its first full year of positive contribution profit in our history. We took a $39m non-cash charge in Q4 for unreleased content we’ve decided not to move forward with. This charge was recognized in content expense in cost of revenues. Despite this unexpected expense, we slightly exceeded our contribution profit and operating income forecast due to our stronger than expected member growth and the timing of international content spend.\"\n",
    "\n",
    "    # pdf_name = \"FINAL-Q1-19-Shareholder-Letter\"\n",
    "\n",
    "    text = pdf_texts[pdf_name]\n",
    "    headings = pdf_headings[pdf_name]\n",
    "\n",
    "    # # split the text into blocks based on the headings\n",
    "    # text_blocks = split_text_into_blocks(text, headings)\n",
    "        \n",
    "    # # clean the text blocks\n",
    "    # text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "    # final_prompt = ''\n",
    "    # for heading, text in text_blocks.items():\n",
    "    #     final_prompt += heading + ': ' + text + \" \"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Chunk the input tensor into smaller parts\n",
    "    max_chunk_size = 512\n",
    "    chunked_input_ids = []\n",
    "    for i in range(0, inputs.input_ids.size(1), max_chunk_size):\n",
    "        start = i\n",
    "        end = min(i + max_chunk_size, inputs.input_ids.size(1))\n",
    "        chunked_input_ids.append(inputs.input_ids[:, start:end])\n",
    "\n",
    "    # Make predictions for each chunk\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunked_input_ids:\n",
    "            outputs = model(input_ids=chunk)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            probs_list.append(probs)\n",
    "\n",
    "    # Combine the probabilities for all chunks\n",
    "    combined_probs = torch.cat(probs_list, dim=0)\n",
    "\n",
    "    # Calculate the average probability for each sentiment class\n",
    "    average_probs = combined_probs.mean(dim=0)\n",
    "    positive_sentiment_prob = average_probs[1].item()\n",
    "    negative_sentiment_prob = average_probs[0].item()\n",
    "\n",
    "    # Determine the sentiment label based on the probability\n",
    "    sentiment_label = \"Positive\" if positive_sentiment_prob > negative_sentiment_prob else \"Negative\"\n",
    "\n",
    "    # Store the results\n",
    "    sentiment_labels[pdf_name] = sentiment_label\n",
    "    positive_sentiment_probs[pdf_name] = positive_sentiment_prob\n",
    "    negative_sentiment_probs[pdf_name] = negative_sentiment_prob\n",
    "\n",
    "    # Print the results\n",
    "    # print(\"Input Text:\", final_prompt)\n",
    "    # print(\"Sentiment Label:\", sentiment_label)\n",
    "    # print(\"Positive Sentiment Probability:\", positive_sentiment_prob)\n",
    "    # print(\"Negative Sentiment Probability:\", negative_sentiment_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_sentiment_labels_df = pd.DataFrame(list(roberta_sentiment_labels.items()), columns=['pdf_name', 'sentiment_label'])\n",
    "roberta_positive_sentiment_probs_df = pd.DataFrame(list(roberta_positive_sentiment_probs.items()), columns=['pdf_name', 'positive_sentiment_prob'])\n",
    "roberta_negative_sentiment_probs_df = pd.DataFrame(list(roberta_negative_sentiment_probs.items()), columns=['pdf_name', 'negative_sentiment_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv file\n",
    "roberta_sentiment_labels_df.to_csv(\"Scores/roberta_sentiment_labels.csv\", index=False)\n",
    "roberta_positive_sentiment_probs_df.to_csv(\"Scores/roberta_positive_sentiment_probs.csv\", index=False)\n",
    "roberta_negative_sentiment_probs_df.to_csv(\"Scores/roberta_negative_sentiment_probs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
