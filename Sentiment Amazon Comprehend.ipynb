{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import openai\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load local modules\n",
    "from module_text_blocks import split_text_into_blocks, clean_text_blocks\n",
    "\n",
    "# Import credentials\n",
    "from credentials_amazon import *\n",
    "from credentials_openai import *\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Connect to Amazon API\n",
    "import boto3\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "os.environ[\"AWS_REGION\"] = AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_analyze_sentiment(text):\n",
    "    comprehend = boto3.client(service_name='comprehend', region_name=\"us-west-2\")\n",
    "    sentiment_response = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "    return sentiment_response[\"SentimentScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_chars(text, num_chars):\n",
    "    \"\"\"Split the input text every num_chars characters.\"\"\"\n",
    "    return [text[i:i+num_chars] for i in range(0, len(text), num_chars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# define the number of tokens in the prompt\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_text_blocks(text):\n",
    "\n",
    "    # for heading, text in text_blocks.items():\n",
    "        \n",
    "    text_length = len(text.split(' '))                           # number of words in the text block\n",
    "    tokens_number = num_tokens_from_string(text, \"cl100k_base\")  # number of tokens in the text block\n",
    "    chars_number = len(text)                                     # number of characters in the text block\n",
    "                    \n",
    "    summarization_blocks = [text]                                # list of text blocks to summarize\n",
    "    responses = []\n",
    "\n",
    "    # if the block contains over 750 words, summarize it\n",
    "    if text_length > 750:\n",
    "\n",
    "        # if the block is exceeding the token limit, split it into multiple blocks\n",
    "        if tokens_number > 3500:\n",
    "\n",
    "            text_split_threshold = int(chars_number / (tokens_number / 2500))\n",
    "            summarization_blocks = split_text_by_chars(text, text_split_threshold)\n",
    "            \n",
    "        for summarization_block in summarization_blocks:\n",
    "\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Please effectively summarize the following text: \" + summarization_block}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500\n",
    "            )                \n",
    "            # top_p=1, frequency_penalty=0, presence_penalty=0, stop=[\"\\n\"]\n",
    "            # add the summarized text to the list of responses\n",
    "            responses.append(completion.choices[0].message.content)\n",
    "\n",
    "        # join the responses into a single text block\n",
    "        # text_blocks[heading] = ' '.join(responses)\n",
    "        text = ' '.join(responses)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf text and headings from the pickle file\n",
    "pdf_texts = pickle.load(open(\"pdf_texts.pkl\", \"rb\"))\n",
    "pdf_headings = pickle.load(open(\"pdf_headings.pkl\", \"rb\"))\n",
    "pdf_headings_context = pickle.load(open(\"pdf_headings_context.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b42743bb0d4626936163ae0bad3f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- No headings found, splitting text into blocks of 4000 characters\n",
      "[0.8027369379997253, 0.5318388342857361, 0.9530623555183411, 0.7993974089622498, 0.4290222227573395, 0.6213720440864563, 0.8690286874771118, 0.9740785956382751, 0.9556066989898682]\n",
      "0.7706826428572336\n"
     ]
    }
   ],
   "source": [
    "polarity_scores = {}\n",
    "# pdf_lists = [\"FINAL-Q1-19-Shareholder-Letter\"]\n",
    "# pdf_lists = [\"FINAL-Q2-20-Shareholder-Letter-V3-with-Tables\"]\n",
    "\n",
    "pdf_lists = [\"Investor-Letter-Q3-2011\"] # check this one\n",
    "\n",
    "for pdf_name in tqdm(pdf_lists):\n",
    "\n",
    "    try:\n",
    "\n",
    "        text_blocks_scores = []\n",
    "\n",
    "        text = pdf_texts[pdf_name]\n",
    "        headings = pdf_headings[pdf_name]\n",
    "        headings_context = pdf_headings_context[pdf_name]\n",
    "\n",
    "        # split the text into blocks based on the headings\n",
    "        text_blocks = split_text_into_blocks(text, headings, headings_context)\n",
    "        # clean the text blocks\n",
    "        text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "        # iterate over the text blocks individually, otherwise single request with all text will fail\n",
    "        if len(headings) > 0:\n",
    "\n",
    "            for heading, text_block in text_blocks.items():\n",
    "\n",
    "                if heading == \"Reference\":\n",
    "                    break\n",
    "\n",
    "                if len(text_block) == 0:\n",
    "                    continue\n",
    "\n",
    "                print(len(text))\n",
    "\n",
    "                # the prompt for amazon sentiment analysis should be less than 5000 bytes\n",
    "                if len(text_block) < 4750:\n",
    "                    polarity_score = amazon_analyze_sentiment(text_block)\n",
    "                    key = max(polarity_score, key=polarity_score.get)\n",
    "                    text_blocks_scores.append(polarity_score[key])\n",
    "                \n",
    "                # split into multi blocks if the text is too long\n",
    "                else:\n",
    "                    # text_blocks = split_text_by_chars(text, 4500)\n",
    "                    text_block = summarize_long_text_blocks(text_block)\n",
    "                    # for text in text_blocks:\n",
    "                    polarity_score = amazon_analyze_sentiment(text_block)\n",
    "                    key = max(polarity_score, key=polarity_score.get)\n",
    "                    text_blocks_scores.append(polarity_score[key])\n",
    "\n",
    "        # if there are no headings, just split the text into block of 4000 characters\n",
    "        else:\n",
    "            print(\"--- No headings found, splitting text into blocks of 4000 characters\")\n",
    "            text_blocks = split_text_by_chars(text, 4000)\n",
    "            for text in text_blocks:\n",
    "                polarity_score = amazon_analyze_sentiment(text)\n",
    "                key = max(polarity_score, key=polarity_score.get)\n",
    "                text_blocks_scores.append(polarity_score[key])\n",
    "                \n",
    "        \n",
    "        polarity_scores[pdf_name] = np.mean(text_blocks_scores)\n",
    "        print(text_blocks_scores)\n",
    "        print(np.mean(text_blocks_scores))\n",
    "\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Exception occurred in file {pdf_name}\")\n",
    "        print(f\"Exception message: {str(e)}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_polarity = pd.DataFrame(list(polarity_scores.items()), columns=['pdf_name', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMBINED-Q4-17-Shareholder-Letter-FINAL</td>\n",
       "      <td>0.905803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FINAL-Q1-18-Shareholder-Letter</td>\n",
       "      <td>0.874805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FINAL-Q1-19-Shareholder-Letter</td>\n",
       "      <td>0.784734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FINAL-Q1-20-Shareholder-Letter</td>\n",
       "      <td>0.809253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FINAL-Q1-21-Shareholder-Letter</td>\n",
       "      <td>0.865836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  pdf_name  polarity\n",
       "0  COMBINED-Q4-17-Shareholder-Letter-FINAL  0.905803\n",
       "1           FINAL-Q1-18-Shareholder-Letter  0.874805\n",
       "2           FINAL-Q1-19-Shareholder-Letter  0.784734\n",
       "3           FINAL-Q1-20-Shareholder-Letter  0.809253\n",
       "4           FINAL-Q1-21-Shareholder-Letter  0.865836"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_polarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "amazon_polarity.to_csv(\"Scores/amazon_polarity.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
