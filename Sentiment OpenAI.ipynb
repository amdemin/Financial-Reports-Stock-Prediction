{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import openai\n",
    "\n",
    "# Load local modules\n",
    "from module_text_blocks import split_text_into_blocks, clean_text_blocks\n",
    "\n",
    "# load credentials for OpenAI API\n",
    "from credentials_openai import *\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_long_text_blocks(text_blocks):\n",
    "\n",
    "    for heading, text in text_blocks.items():\n",
    "        # if the block is too long (contains over 750 words), summarize it\n",
    "        if len(text.split(' ')) > 750:\n",
    "\n",
    "            # use the OpenAI API to summarize the text\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Please effectively summarize the following text: \" + text}\n",
    "                ])\n",
    "            # replace the long text block with the summarized version\n",
    "            text_blocks[heading] = completion.choices[0].message.content\n",
    "    \n",
    "    return text_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_sentiment_analysis(final_prompt):\n",
    "\n",
    "    # run the request for ChatGPT\n",
    "    fine_tune_messages = {\"role\": \"system\", \"content\":\n",
    "                    \"You are a helpful financial assistant who is expert in evaluating sentiment scores for financial statements \\\n",
    "                You give precise answers to questions \\\n",
    "                the quality of your answers is highly important, you never hallucinate answers - only \\\n",
    "                answering based on your knowledge. Where the answer requires creative thought you engage \\\n",
    "                in reflective internal dialogue to ascertain the best answer\"\n",
    "    }\n",
    "\n",
    "    user_content = \"Calculate the total polarity and subjectivity scores on the strict range -1 to 1 (-1 means perfectly negative; 1 means perfectly positive): \"\n",
    "\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        fine_tune_messages,\n",
    "        {\"role\": \"user\", \"content\": user_content + final_prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_sentiment_analysis_finetune(final_prompt, response):\n",
    "\n",
    "    # run the request for ChatGPT\n",
    "    fine_tune_messages = {\"role\": \"system\", \"content\":\n",
    "                    \"You are a helpful financial assistant who is expert in evaluating sentiment scores for financial statements \\\n",
    "                You give precise answers to questions \\\n",
    "                the quality of your answers is highly important, you never hallucinate answers - only \\\n",
    "                answering based on your knowledge. Where the answer requires creative thought you engage \\\n",
    "                in reflective internal dialogue to ascertain the best answer\"\n",
    "    }\n",
    "\n",
    "    user_content = \"Calculate the total polarity and subjectivity scores on the strict range -1 to 1 (-1 means perfectly negative; 1 means perfectly positive): \"\n",
    "\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        fine_tune_messages,\n",
    "        {\"role\": \"user\", \"content\": user_content + final_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response},\n",
    "        {\"role\": \"user\", \"content\": \"No, you don't understand, tell me only two values, the total polarity score of report on range from -1 to 1 and the total subjectivity score of report on range from -1 to 1. the text is the following:\" + final_prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_chars(text, num_chars):\n",
    "    \"\"\"Split the input text every num_chars characters.\"\"\"\n",
    "    return [text[i:i+num_chars] for i in range(0, len(text), num_chars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf text and headings from the pickle file\n",
    "pdf_texts = pickle.load(open(\"pdf_texts.pkl\", \"rb\"))\n",
    "pdf_headings = pickle.load(open(\"pdf_headings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfd4be2680344bab44b684e9290dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store openai responses in a dictionary\n",
    "openai_responses = {}\n",
    "fine_tuned_responses = {}\n",
    "\n",
    "for pdf_name in tqdm(pdf_texts):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # pdf_name = \"Investor_Letter_Q12013\"\n",
    "\n",
    "        text = pdf_texts[pdf_name]\n",
    "        headings = pdf_headings[pdf_name]\n",
    "\n",
    "        # split the text into blocks based on the headings\n",
    "        text_blocks = split_text_into_blocks(text, headings)\n",
    "        \n",
    "        # clean the text blocks\n",
    "        text_blocks = clean_text_blocks(text_blocks)\n",
    "\n",
    "        if len(headings) > 0:\n",
    "            \n",
    "            # print the original length of the text blocks\n",
    "            # print(\"Original length of blocks for \" + pdf_name + \":\")\n",
    "            # for heading, text in text_blocks.items():\n",
    "            #     print(len(text.split(\" \")), end=\" \")\n",
    "            # print(\" \")\n",
    "\n",
    "            # summarize the text blocks\n",
    "            text_blocks = summarize_long_text_blocks(text_blocks)\n",
    "\n",
    "            # print the length of the text blocks after summarization\n",
    "            # print(\"Updated length of blocks for \" + pdf_name + \":\")\n",
    "            # for heading, text in text_blocks.items():\n",
    "            #     print(len(text.split(\" \")), end=\" \")\n",
    "            # print(\" \")\n",
    "\n",
    "            # Create a final prompt\n",
    "            final_prompt = ''\n",
    "            for heading, text in text_blocks.items():\n",
    "                final_prompt += heading + ': ' + text + \" \"\n",
    "        else:\n",
    "            final_prompt = text[:25000]\n",
    "\n",
    "            \n",
    "\n",
    "        # Perform openAI sentiment analysis\n",
    "        response = openai_sentiment_analysis(final_prompt)\n",
    "        openai_responses[pdf_name] = response\n",
    "\n",
    "        # response = openai_sentiment_analysis_finetune(final_prompt, response)\n",
    "        # fine_tuned_responses[pdf_name] = response\n",
    "\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred in file {pdf_name}\")\n",
    "        print(f\"Exception message: {str(e)}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# define the number of tokens in the prompt\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2660"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show number of tokens in prompt (max 4096 tokens for both input & output)\n",
    "num_tokens_from_string(final_prompt, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save openai responses to pickle file\n",
    "with open(\"Src/openai_responses_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(openai_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load openai responses from the pickle file\n",
    "openai_responses = pickle.load(open(\"Src/openai_responses_3.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_responses_df_2 = pd.DataFrame(list(openai_responses.items()), columns=['pdf_name', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OpenAI responses to excel\n",
    "openai_responses_df_2.to_excel('Src/openai_responses_df_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text of the responses\n",
    "for key in openai_responses:\n",
    "        openai_responses[key] = re.sub('\\n', ' ', openai_responses[key])\n",
    "        openai_responses[key] = re.sub(' +', ' ', openai_responses[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract polarity and subjectivity scores from OpenAI responses\n",
    "polarity_scores = {}\n",
    "subjectivity_scores = {}\n",
    "\n",
    "\n",
    "for document, text in openai_responses.items():\n",
    "\n",
    "    words = text.split(\" \")\n",
    "    for word in words:\n",
    "        if word.lower() == \"polarity\":\n",
    "            polarity_scores[document] = words[words.index(word):words.index(word) + 4]\n",
    "        elif word.lower() == \"subjectivity\":\n",
    "            subjectivity_scores[document] = words[words.index(word):words.index(word) + 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract digits from polarity_scores dictionary\n",
    "for document, words in polarity_scores.items():\n",
    "\n",
    "    for word in words:\n",
    "        # if word is number using regex\n",
    "        if re.search(\"^[0-9]\", word):\n",
    "            word = word.replace(\",\", \"\")\n",
    "            polarity_scores[document] = float(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract digits from subjectivity scores dictionary\n",
    "for document, words in subjectivity_scores.items():\n",
    "\n",
    "    for word in words:\n",
    "        # if word is number using regex\n",
    "        if re.search(\"^[0-9]\", word):\n",
    "            word = word.replace(\",\", \"\")\n",
    "            subjectivity_scores[document] = float(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out keys values of which are lists\n",
    "polarity_scores = {k: v for k, v in polarity_scores.items() if type(v) != list}\n",
    "subjectivity_scores = {k: v for k, v in subjectivity_scores.items() if type(v) != list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(pdf_texts.items()), columns=['pdf_name', 'text'])\n",
    "polarity_df = pd.DataFrame(list(polarity_scores.items()), columns=['pdf_name', 'polarity'])\n",
    "subjectivity_df = pd.DataFrame(list(subjectivity_scores.items()), columns=['pdf_name', 'subjectivity'])\n",
    "\n",
    "# join df and polarity_df on pdf_name column\n",
    "df = df.join(polarity_df.set_index(\"pdf_name\"), on=\"pdf_name\")\n",
    "df = df.join(subjectivity_df.set_index(\"pdf_name\"), on=\"pdf_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export df\n",
    "df.to_excel('Score/OpenAI_scores.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
